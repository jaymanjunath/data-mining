{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TwitterMining\n",
    "\n",
    "Goal: Predict the animal classification of tweets (cat or dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import modules\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import text as sk_fe_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score\n",
    "from TwitterAPI import TwitterAPI\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Twitter api key\n",
    "api = TwitterAPI('QNfuwJhdHesrXfpVmYGYx9UYi', '8ZTHg0osCzwOqxvmqwZUNI1KaWu2PI9C4tao29VeEp0UlzlTbH', auth_type = 'oAuth2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(true,pred):\n",
    "    return (precision_score(true,pred),\n",
    "            recall_score(true,pred),\n",
    "            f1_score(true,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_score(s):\n",
    "    print (\"\"\"\n",
    "Precision: {:0.3}\n",
    "Recall:    {:0.3}\n",
    "F-SCore:   {:0.3}\n",
    "\"\"\".format(*s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def searchTwitter(query,feed=\"search/tweets\",api=api,n=4000):\n",
    "  r = []\n",
    "  qs = 0\n",
    "  if len(r)==0:\n",
    "    r.extend([t for t in api.request(\"search/tweets\",{'q':query,'count':n})])\n",
    "    qs +=1\n",
    "  while len(r) < n:\n",
    "#     print(\"Querrying twitter for {}. {}/{} gathered.\".format(query,len(r),n))\n",
    "    last = r[-1]['id']\n",
    "    r.extend([t for t in api.request(\"search/tweets\",{'q':query,'count':n,\n",
    "                                                        'max_id':last})])\n",
    "    qs += 1\n",
    "    if qs > 180:\n",
    "      time.sleep(840)\n",
    "      qs = 0\n",
    "  return r[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    words = []\n",
    "    for line in tweet.split():\n",
    "        line = line.lower()\n",
    "        # allow hashtags and account handles\n",
    "        if line == '#dogs' or line == '#cats':\n",
    "            pass\n",
    "        elif line.startswith('#') or line.startswith('@'):\n",
    "            # remove non-alphanumeric characters at the end of handle (if present)\n",
    "            if line[-1].isalpha():\n",
    "                #print('\\t✅ '+ line)\n",
    "                words.append(line)\n",
    "            else:\n",
    "                #print('\\t✅ '+ line[0:-1])\n",
    "                words.append(line[0:-1])\n",
    "        # ignore non-alphanumeric values, links, and retweets\n",
    "        elif not line.isalpha() or line.startswith('http') or line=='RT':\n",
    "            #print('\\t❌ ' + line)\n",
    "            pass\n",
    "        else:\n",
    "            #print('\\t✅ '+ line)\n",
    "            words.append(line)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_dictionary_word_count(dictionary, words):\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            dictionary[word] = dictionary[word] + 1\n",
    "        else:\n",
    "            dictionary[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_dict_tweet_weight(tweet, dictionary):\n",
    "    weight = 0\n",
    "    for line in tweet.split():\n",
    "        if line.startswith('#'):\n",
    "            if line[-1].isalpha():\n",
    "                if line in dictionary:\n",
    "                    weight = weight + dictionary[line]\n",
    "            else:\n",
    "                if line[0:-1] in dictionary:\n",
    "                    weight = weight + dictionary[line[0:-1]]\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**Note:** The code below may take a while (~45 seconds) as it is searching for Tweets_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tweets from Twitter\n",
    "cats = searchTwitter('#cats')\n",
    "dogs = searchTwitter('#dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting json from Twitter into a dataframe\n",
    "cats_d = pd.read_json(json.dumps(cats))\n",
    "dogs_d = pd.read_json(json.dumps(dogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(cats_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(dogs_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt #1: Word Count\n",
    "\n",
    "The first attempt will be to count the word frequency in both classes of Tweets. The hope is if there is a distinct pattern of words used for the two Tweet classes.\n",
    "\n",
    "Properties of the cleaned Tweet include:\n",
    "* Track the accounts mentioned.\n",
    "* Track the hashtags (ignoring the one classification tweet).\n",
    "* Omission of both '#cats' and '#dogs' in the same tweet.\n",
    "* Omission of non-alphanumeric characters\n",
    "\n",
    "Note to self: dismissing data is already putting a bias on the results. Who is to say that cat lovers don't like adding special characters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #cats word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count word frequency for '#cats'\n",
    "index = 0\n",
    "cats_dict = {}\n",
    "sliced_tweet = []\n",
    "while index < 4000:\n",
    "    sliced_tweet = clean_tweet(cats_d.iloc[index]['text'])\n",
    "    update_dictionary_word_count(cats_dict, sliced_tweet)\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print top words found in '#cats'\n",
    "popular_cat_words = sorted(cats_dict.values())[::-1][:20]\n",
    "for key in cats_dict.keys():\n",
    "    if cats_dict[key] in popular_cat_words:\n",
    "        print(key + ':' + str(cats_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #dogs word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count word frequency for '#dogs'\n",
    "index = 0\n",
    "dogs_dict = {}\n",
    "sliced_tweet = []\n",
    "while index < 4000:    \n",
    "    # dogs\n",
    "    sliced_tweet = clean_tweet(dogs_d.iloc[index]['text'])\n",
    "    update_dictionary_word_count(dogs_dict, sliced_tweet)\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print top words found in '#cats'\n",
    "popular_dog_words = sorted(dogs_dict.values())[::-1][:20]\n",
    "for key in dogs_dict.keys():\n",
    "    if dogs_dict[key] in popular_dog_words:\n",
    "        print(key + ':' + str(dogs_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "cats_hit_count = 0\n",
    "cats_miss_count = 0\n",
    "dogs_hit_count = 0\n",
    "dogs_miss_count = 0\n",
    "\n",
    "while index < 4000:\n",
    "    # cats\n",
    "    cats_cats = calc_dict_tweet_weight(cats_d.iloc[index]['text'], cats_dict)\n",
    "    cats_dogs = calc_dict_tweet_weight(cats_d.iloc[index]['text'], dogs_dict)\n",
    "    if cats_cats > cats_dogs:\n",
    "        cats_hit_count = cats_hit_count + 1\n",
    "    else:\n",
    "        cats_miss_count = cats_miss_count + 1\n",
    "\n",
    "    # dogs\n",
    "    dogs_cats = calc_dict_tweet_weight(dogs_d.iloc[index]['text'], cats_dict)\n",
    "    dogs_dogs = calc_dict_tweet_weight(dogs_d.iloc[index]['text'], dogs_dict)\n",
    "    if dogs_dogs > dogs_cats:\n",
    "        dogs_hit_count = dogs_hit_count + 1\n",
    "    else:\n",
    "        dogs_miss_count = dogs_miss_count + 1\n",
    "\n",
    "    index = index + 1\n",
    "    \n",
    "print('cats correct: ' + str(cats_hit_count))\n",
    "print('cats incorrect: ' + str(cats_miss_count))\n",
    "print('dogs correct: ' + str(dogs_hit_count))\n",
    "print('dogs incorrect: ' + str(dogs_miss_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data shows, it's roughly a 50% prediction rate, which is as good as guessing. \n",
    "\n",
    "A better method should be to use the bag-of-words model. This way, machine learning is used to classify the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt #2: bag-of-words with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Will be using [this](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a) article for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get text only and replace  hashtags with blanks\n",
    "cats_text = [x.replace(\"#cats\", \"\") for x in cats_d['text']]\n",
    "dogs_text = [x.replace(\"#dogs\", \"\") for x in dogs_d['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the number of times #dogs appear in the cats data\n",
    "blob = [x.find(\"#dogs\") for x in cats_text]\n",
    "type(blob)\n",
    "df1 = pd.DataFrame(blob)\n",
    "#df1.stack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create features and return sparse metrics \n",
    "vectorizer = sk_fe_text.CountVectorizer(cats_text+dogs_text)\n",
    "vectorizer.fit(cats_text+dogs_text)\n",
    "cats_tdm = vectorizer.transform(cats_text).toarray()\n",
    "dogs_tdm = vectorizer.transform(dogs_text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create visible matrices, combine and add the number of times #dogs appear in cats_text to the length of dog_text and subtract\n",
    "#the same number from the length of cats_text\n",
    "zeros = np.zeros((len(cats_text) - 185, 1))\n",
    "ones = np.ones((len(dogs_text) + 185, 1))\n",
    "catsdogs = np.concatenate((cats_tdm,dogs_tdm),axis=0)\n",
    "y = np.ravel(np.concatenate((zeros,ones),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train/test split for modeling\n",
    "trainX,testX,trainY,testY = train_test_split(catsdogs,y,test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Naive Bayes Performance\n",
      "\n",
      "Precision: 0.939\n",
      "Recall:    0.755\n",
      "F-SCore:   0.837\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes cont'd\n",
    "print(\"\\n\\nNaive Bayes Performance\")\n",
    "s = score(testY,nb.predict(testX))\n",
    "print_score(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier()\n",
    "nn.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Neural Network Performance\n",
      "\n",
      "Precision: 0.908\n",
      "Recall:    0.928\n",
      "F-SCore:   0.918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Neural Network cont'd\n",
    "print(\"\\n\\nNeural Network Performance\")\n",
    "s = score(testY,nn.predict(testX))\n",
    "print_score(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
